{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "apld_data = pd.read_csv('../data/processed data/APLD_merged.csv')\n",
    "cney_data = pd.read_csv('../data/processed data/CNEY_merged.csv')\n",
    "ktta_data = pd.read_csv('../data/processed data/KTTA_merged.csv')\n",
    "onco_data = pd.read_csv('../data/processed data/ONCO_merged.csv')\n",
    "tnxp_data = pd.read_csv('../data/processed data/TNXP_merged.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest Classifer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5374149659863946\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.48      0.48      0.48        66\n",
      "           1       0.58      0.58      0.58        81\n",
      "\n",
      "    accuracy                           0.54       147\n",
      "   macro avg       0.53      0.53      0.53       147\n",
      "weighted avg       0.54      0.54      0.54       147\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Creating the target variable (1: price increase, 0: price decrease)\n",
    "def create_target_variable(df, close_col):\n",
    "    df['Price_Change'] = (df[close_col].diff() > 0).astype(int)\n",
    "    df = df.dropna()  # Drop null values \n",
    "    return df\n",
    "\n",
    "apld_data = create_target_variable(apld_data, 'APLD_Close')\n",
    "cney_data = create_target_variable(cney_data, 'CNEY_Close')\n",
    "ktta_data = create_target_variable(ktta_data, 'KTTA_Close')\n",
    "onco_data = create_target_variable(onco_data, 'ONCO_Close')\n",
    "tnxp_data = create_target_variable(tnxp_data, 'TNXP_Close')\n",
    "\n",
    "# Sentiment, Volume, and Previous Day's Price\n",
    "def prepare_features(df, close_col, volume_col, sentiment_col):\n",
    "    df['Prev_Close'] = df[close_col].shift(1)\n",
    "    df = df.dropna()  # Drop rows with NaN values \n",
    "    X = df[[sentiment_col, volume_col, 'Prev_Close']]\n",
    "    y = df['Price_Change']\n",
    "    return X, y\n",
    "\n",
    "X_apld, y_apld = prepare_features(apld_data, 'APLD_Close', 'APLD_Volume', 'Sentiment')\n",
    "X_cney, y_cney = prepare_features(cney_data, 'CNEY_Close', 'CNEY_Volume', 'Sentiment')\n",
    "X_ktta, y_ktta = prepare_features(ktta_data, 'KTTA_Close', 'KTTA_Volume', 'Sentiment')\n",
    "X_onco, y_onco = prepare_features(onco_data, 'ONCO_Close', 'ONCO_Volume', 'Sentiment')\n",
    "X_tnxp, y_tnxp = prepare_features(tnxp_data, 'TNXP_Close', 'TNXP_Volume', 'Sentiment')\n",
    "\n",
    "X_combined = pd.concat([X_apld, X_cney, X_ktta, X_onco, X_tnxp], axis=0)\n",
    "y_combined = pd.concat([y_apld, y_cney, y_ktta, y_onco, y_tnxp], axis=0)\n",
    "\n",
    "X_combined = X_combined.assign(Prev_Close=X_combined['Prev_Close'].fillna(X_combined['Prev_Close'].mean()))\n",
    "\n",
    "X_train_imputed, X_test_imputed, y_train_imputed, y_test_imputed = train_test_split(X_combined, y_combined, test_size=0.3)\n",
    "\n",
    "# Training: Random Forest Classifier\n",
    "model_imputed = RandomForestClassifier()\n",
    "model_imputed.fit(X_train_imputed, y_train_imputed)\n",
    "\n",
    "# Predictions\n",
    "y_pred_imputed = model_imputed.predict(X_test_imputed)\n",
    "\n",
    "# Model Evaluation\n",
    "accuracy_imputed = accuracy_score(y_test_imputed, y_pred_imputed)\n",
    "classification_rep_imputed = classification_report(y_test_imputed, y_pred_imputed)\n",
    "\n",
    "print(f\"Accuracy: {accuracy_imputed}\")\n",
    "print(\"Classification Report:\\n\", classification_rep_imputed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Accuracy:*** 53.74%\n",
    "\n",
    "This means the model correctly predicted whether the stock price would increase or decrease 53.95% of the time, which is only slightly better than random guessing (50%).\n",
    "\n",
    "***Precision:***\n",
    "\n",
    "- **Class 0 (Price Decrease):** The model predicted a price decrease correctly 48% of the time.\n",
    "\n",
    "- **Class 1 (Price Increase):** The model predicted a price increase correctly 58% of the time. This indicates that the model performs better at identifying price increases.\n",
    "\n",
    "***Recall:***\n",
    "\n",
    "- **Class 0 (Price Decrease):** 48% recall means that of all the actual price decreases, the model correctly identified 48% of them.\n",
    "\n",
    "- **Class 1 (Price Increase):** 58% recall means that of all the actual price increases, the model correctly identified 58%.\n",
    "\n",
    "***F1-Score:***\n",
    "\n",
    "- **Class 0 (Price Decrease):** F1-score of 48% indicates a balance between precision and recall for price decreases. The model struggles more with predicting decreases.\n",
    "\n",
    "- **Class 1 (Price Increase):** F1-score of 58% shows a better balance for price increases.\n",
    "\n",
    "***Overall Interpretation:***\n",
    "\n",
    "- The model is better at predicting price increases (Class 1) than decreases (Class 0). This could be happening because the datasets have an imbalance and certain patterns are more easily detectable for one class.\n",
    "\n",
    "- The model has moderate performance but struggles with false positives for Class 0, where it incorrectly predicts price decreases more often."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression & Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "APLD - Logistic Regression: 0.5, Gradient Boosting: 0.5\n",
      "CNEY - Logistic Regression: 0.59375, Gradient Boosting: 0.46875\n",
      "KTTA - Logistic Regression: 0.4117647058823529, Gradient Boosting: 0.47058823529411764\n",
      "ONCO - Logistic Regression: 0.5833333333333334, Gradient Boosting: 0.5416666666666666\n",
      "TNXP - Logistic Regression: 0.8, Gradient Boosting: 0.4\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def prepare_stock_data(df, stock_name):\n",
    "    df['Date'] = pd.to_datetime(df['Date'])  \n",
    "    df = df.sort_values('Date')  \n",
    "    \n",
    "    # Create binary target: 1 if next day's price increased, 0 otherwise\n",
    "    df['Target'] = (df[f'{stock_name}_Close'].shift(-1) > df[f'{stock_name}_Close']).astype(int)\n",
    "    df = df.dropna()  # Drop rows with NaN values \n",
    "    \n",
    "    features = [f'{stock_name}_Close', f'{stock_name}_Volume', 'Sentiment']\n",
    "    X = df[features]\n",
    "    y = df['Target']\n",
    "    \n",
    "    return X, y, df\n",
    "\n",
    "X_apld, y_apld, apld_prepared = prepare_stock_data(apld_data, 'APLD')\n",
    "X_cney, y_cney, cney_prepared = prepare_stock_data(cney_data, 'CNEY')\n",
    "X_ktta, y_ktta, ktta_prepared = prepare_stock_data(ktta_data, 'KTTA')\n",
    "X_onco, y_onco, onco_prepared = prepare_stock_data(onco_data, 'ONCO')\n",
    "X_tnxp, y_tnxp, tnxp_prepared = prepare_stock_data(tnxp_data, 'TNXP')\n",
    "\n",
    "# Train and test sets for each stock\n",
    "X_train_apld, X_test_apld, y_train_apld, y_test_apld = train_test_split(X_apld, y_apld, test_size=0.2, shuffle=False)\n",
    "X_train_cney, X_test_cney, y_train_cney, y_test_cney = train_test_split(X_cney, y_cney, test_size=0.2, shuffle=False)\n",
    "X_train_ktta, X_test_ktta, y_train_ktta, y_test_ktta = train_test_split(X_ktta, y_ktta, test_size=0.2, shuffle=False)\n",
    "X_train_onco, X_test_onco, y_train_onco, y_test_onco = train_test_split(X_onco, y_onco, test_size=0.2, shuffle=False)\n",
    "X_train_tnxp, X_test_tnxp, y_train_tnxp, y_test_tnxp = train_test_split(X_tnxp, y_tnxp, test_size=0.2, shuffle=False)\n",
    "\n",
    "# Standardize the features for logistic regression\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled_apld = scaler.fit_transform(X_train_apld)\n",
    "X_test_scaled_apld = scaler.transform(X_test_apld)\n",
    "\n",
    "X_train_scaled_cney = scaler.fit_transform(X_train_cney)\n",
    "X_test_scaled_cney = scaler.transform(X_test_cney)\n",
    "\n",
    "X_train_scaled_ktta = scaler.fit_transform(X_train_ktta)\n",
    "X_test_scaled_ktta = scaler.transform(X_test_ktta)\n",
    "\n",
    "X_train_scaled_onco = scaler.fit_transform(X_train_onco)\n",
    "X_test_scaled_onco = scaler.transform(X_test_onco)\n",
    "\n",
    "X_train_scaled_tnxp = scaler.fit_transform(X_train_tnxp)\n",
    "X_test_scaled_tnxp = scaler.transform(X_test_tnxp)\n",
    "\n",
    "# Logistic Regression \n",
    "log_reg_apld = LogisticRegression()\n",
    "log_reg_apld.fit(X_train_scaled_apld, y_train_apld)\n",
    "y_pred_log_reg_apld = log_reg_apld.predict(X_test_scaled_apld)\n",
    "\n",
    "log_reg_cney = LogisticRegression()\n",
    "log_reg_cney.fit(X_train_scaled_cney, y_train_cney)\n",
    "y_pred_log_reg_cney = log_reg_cney.predict(X_test_scaled_cney)\n",
    "\n",
    "log_reg_ktta = LogisticRegression()\n",
    "log_reg_ktta.fit(X_train_scaled_ktta, y_train_ktta)\n",
    "y_pred_log_reg_ktta = log_reg_ktta.predict(X_test_scaled_ktta)\n",
    "\n",
    "log_reg_onco = LogisticRegression()\n",
    "log_reg_onco.fit(X_train_scaled_onco, y_train_onco)\n",
    "y_pred_log_reg_onco = log_reg_onco.predict(X_test_scaled_onco)\n",
    "\n",
    "log_reg_tnxp = LogisticRegression()\n",
    "log_reg_tnxp.fit(X_train_scaled_tnxp, y_train_tnxp)\n",
    "y_pred_log_reg_tnxp = log_reg_tnxp.predict(X_test_scaled_tnxp)\n",
    "\n",
    "# Gradient Boosting \n",
    "gbc_apld = GradientBoostingClassifier()\n",
    "gbc_apld.fit(X_train_apld, y_train_apld)\n",
    "y_pred_gbc_apld = gbc_apld.predict(X_test_apld)\n",
    "\n",
    "gbc_cney = GradientBoostingClassifier()\n",
    "gbc_cney.fit(X_train_cney, y_train_cney)\n",
    "y_pred_gbc_cney = gbc_cney.predict(X_test_cney)\n",
    "\n",
    "gbc_ktta = GradientBoostingClassifier()\n",
    "gbc_ktta.fit(X_train_ktta, y_train_ktta)\n",
    "y_pred_gbc_ktta = gbc_ktta.predict(X_test_ktta)\n",
    "\n",
    "gbc_onco = GradientBoostingClassifier()\n",
    "gbc_onco.fit(X_train_onco, y_train_onco)\n",
    "y_pred_gbc_onco = gbc_onco.predict(X_test_onco)\n",
    "\n",
    "gbc_tnxp = GradientBoostingClassifier()\n",
    "gbc_tnxp.fit(X_train_tnxp, y_train_tnxp)\n",
    "y_pred_gbc_tnxp = gbc_tnxp.predict(X_test_tnxp)\n",
    "\n",
    "# Models for each stock\n",
    "log_reg_accuracy_apld = accuracy_score(y_test_apld, y_pred_log_reg_apld)\n",
    "gbc_accuracy_apld = accuracy_score(y_test_apld, y_pred_gbc_apld)\n",
    "\n",
    "log_reg_accuracy_cney = accuracy_score(y_test_cney, y_pred_log_reg_cney)\n",
    "gbc_accuracy_cney = accuracy_score(y_test_cney, y_pred_gbc_cney)\n",
    "\n",
    "log_reg_accuracy_ktta = accuracy_score(y_test_ktta, y_pred_log_reg_ktta)\n",
    "gbc_accuracy_ktta = accuracy_score(y_test_ktta, y_pred_gbc_ktta)\n",
    "\n",
    "log_reg_accuracy_onco = accuracy_score(y_test_onco, y_pred_log_reg_onco)\n",
    "gbc_accuracy_onco = accuracy_score(y_test_onco, y_pred_gbc_onco)\n",
    "\n",
    "log_reg_accuracy_tnxp = accuracy_score(y_test_tnxp, y_pred_log_reg_tnxp)\n",
    "gbc_accuracy_tnxp = accuracy_score(y_test_tnxp, y_pred_gbc_tnxp)\n",
    "\n",
    "print(f\"APLD - Logistic Regression: {log_reg_accuracy_apld}, Gradient Boosting: {gbc_accuracy_apld}\")\n",
    "print(f\"CNEY - Logistic Regression: {log_reg_accuracy_cney}, Gradient Boosting: {gbc_accuracy_cney}\")\n",
    "print(f\"KTTA - Logistic Regression: {log_reg_accuracy_ktta}, Gradient Boosting: {gbc_accuracy_ktta}\")\n",
    "print(f\"ONCO - Logistic Regression: {log_reg_accuracy_onco}, Gradient Boosting: {gbc_accuracy_onco}\")\n",
    "print(f\"TNXP - Logistic Regression: {log_reg_accuracy_tnxp}, Gradient Boosting: {gbc_accuracy_tnxp}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### APLD\n",
    "- **Logistic Regression Accuracy:** 50%\n",
    "- **Gradient Boosting Accuracy:** 50%\n",
    "  \n",
    "Both Logistic Regression and Gradient Boosting performed equally, with an accuracy of 50%. This suggests that neither model was able to reliably predict price movements for APLD, likely due to data limitations or a lack of clear signal in the features used.\n",
    "\n",
    "### CNEY\n",
    "- **Logistic Regression Accuracy:** 59.4%\n",
    "- **Gradient Boosting Accuracy:** 46.9%\n",
    "\n",
    "Logistic Regression performed better, indicating that the linear relationships captured by this model were more predictive for CNEY. Gradient Boosting, a more complex model, struggled potentially due to overfitting or insufficient feature depth.\n",
    "\n",
    "### KTTA\n",
    "- **Logistic Regression Accuracy:** 41.2%\n",
    "- **Gradient Boosting Accuracy:** 47.1%\n",
    "\n",
    "The performances of both models were close, but neither model performed particularly well. This could indicate that the data for KTTA stock is noisier or more challenging to predict, making it harder for either model to capture meaningful patterns.\n",
    "\n",
    "### ONCO\n",
    "- **Logistic Regression Accuracy:** 58.3%\n",
    "- **Gradient Boosting Accuracy:** 54.2%\n",
    "\n",
    "Both models performed similarly, with Logistic Regression slightly outperforming Gradient Boosting. This suggests that a simpler model like Logistic Regression may be sufficient for this stock, as it captured the key patterns more effectively.\n",
    "\n",
    "### TNXP\n",
    "- **Logistic Regression Accuracy:** 80%\n",
    "- **Gradient Boosting Accuracy:** 40%\n",
    "\n",
    "Logistic Regression significantly outperformed Gradient Boosting. This stark difference suggests that the price movements for TNXP may follow a simpler, more linear pattern, making Logistic Regression a much better fit. Gradient Boosting, being more flexible, may have overfit to noise in the data.\n",
    "\n",
    "\n",
    "### Overall Insights:\n",
    "- **Logistic Regression** handled the data better in most cases, especially for TNXP and CNEY.\n",
    "- **Gradient Boosting** underperformed in several cases, potentially due to overfitting or the limitations of the feature set.\n",
    "- The performance for some stocks (like KTTA and APLD) indicates that more data or additional features, such as better sentiment data, could have improved the models.\n",
    "\n",
    "These results could be attributed to the nature of the sentiment data and its limited scope, which impacts more complex models like Gradient Boosting. This reinforces the observation that richer data would have improved overall performance.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
